# Loan Approval Optimization ‚Äî Deep Learning + Offline Reinforcement Learning

This repository provides a complete end-to-end pipeline for **loan approval optimization** using both **Deep Learning (DL)** and **Offline Reinforcement Learning (RL)**.

The objective is to help a fintech institution maximize profit while minimizing loan defaults by learning from historical data and optimizing the approval policy.

---

## üß≠ Overview

The workflow is divided into four main tasks:

- **Task 1:** Exploratory Data Analysis (EDA) & Preprocessing  
- **Task 2:** Deep Learning default-risk prediction (PyTorch MLP)  
- **Task 3:** Offline Reinforcement Learning (CQL via d3rlpy) for profit-maximizing approvals  
- **Task 4:** Analysis & Comparison ‚Äî evaluating DL (AUC/F1) vs RL (Estimated Policy Value), identifying disagreement cases, and proposing future directions  

You can reproduce all experiments locally (Python 3.11 recommended) or on **Google Colab**.

---

## üìÇ Folder Structure

```
.
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ accepted_2007_to_2018Q4.csv.gz         # Raw dataset (optional)
‚îÇ   ‚îî‚îÄ‚îÄ loan_clean_subset.csv                   # Cleaned dataset (from Task 1)
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ Task1_EDA_Preprocess.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ Task2_DL_Predictive_Model.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ Task3_Offline_RL_CQL.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ Task4_Analysis_Comparison.ipynb         # Combined analysis notebook
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ model_mlp_default_risk/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pytorch_mlp.pt                      # Saved DL model weights
‚îÇ   ‚îî‚îÄ‚îÄ offline_rl_cql/
‚îÇ       ‚îú‚îÄ‚îÄ cql_discrete_model.d3               # Trained RL policy (if supported)
‚îÇ       ‚îî‚îÄ‚îÄ preprocess.joblib                   # Sklearn preprocessor for inference
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

> üí° **Tip:** If you don‚Äôt have the raw CSV, you can still run the pipeline using `data/loan_clean_subset.csv` produced in Task 1.

---

## ‚öôÔ∏è Environment Setup

### **Option A ‚Äî Conda (Recommended)**

> ‚ö†Ô∏è Use **Python 3.11**. Some TensorFlow and RL libraries are not yet compatible with Python 3.13.

```bash
# From the repository root
conda create -n loan-rl python=3.11 -y
conda activate loan-rl

# Install dependencies
python -m pip install -U pip
pip install -r requirements.txt

# (Optional) Add this environment as a Jupyter kernel
python -m ipykernel install --user --name loan-rl --display-name "Python 3.11 (loan-rl)"

# Launch Jupyter Notebook
jupyter notebook
# In the Jupyter UI:  Kernel ‚Üí Change Kernel ‚Üí "Python 3.11 (loan-rl)"
```

---

## üóÉÔ∏è Data Files

Place the following inside the `data/` directory:

- **Preferred:** `loan_clean_subset.csv`  
  ‚Äì Generated by Task 1 after preprocessing  
  ‚Äì Includes features and target column (`0 = Fully Paid`, `1 = Defaulted`)

- **Optional:** `accepted_2007_to_2018Q4.csv.gz`  
  ‚Äì If present, Task 1 automatically creates `loan_clean_subset.csv`

> Ensure that `loan_amnt` and `int_rate` columns exist ‚Äî both are required for RL reward calculations.

---

## üöÄ How to Run the Pipeline

### **Step 1 ‚Äî EDA & Preprocessing**
Open **`notebooks/Task1_EDA_Preprocess.ipynb`** and select **Run All**.  
This notebook:
- Loads the raw CSV (if available)  
- Cleans and engineers features  
- Saves `data/loan_clean_subset.csv`

**Output:**  
`data/loan_clean_subset.csv`

---

### **Step 2 ‚Äî Deep Learning (DL) Model**
Open **`notebooks/Task2_DL_Predictive_Model.ipynb`** and run all cells.  
This notebook:
- Loads `loan_clean_subset.csv`
- Builds a preprocessing pipeline (imputation + OHE + scaling)
- Trains a PyTorch MLP model or loads existing weights
- Evaluates **ROC-AUC** and **F1-Score** on the test set

**Outputs:**
- `models/model_mlp_default_risk/pytorch_mlp.pt`  
- Printed metrics (AUC, F1, classification report, confusion matrix)

---

### **Step 3 ‚Äî Offline Reinforcement Learning (CQL)**
Open **`notebooks/Task3_Offline_RL_CQL.ipynb`** and run all cells.  
This notebook:
- Constructs a bandit-style MDPDataset (duplicating each state with actions {deny, approve})
- Trains a **CQL agent** using **d3rlpy**
- Computes **Estimated Policy Value (EPV)** on the test set

**Outputs:**
- `models/offline_rl_cql/cql_discrete_model.d3` (if supported)  
- `models/offline_rl_cql/preprocess.joblib`  
- Printed EPV, approval rate, and approval breakdown

---

### **Step 4 ‚Äî Analysis & Comparison**
Open **`notebooks/Task4_Analysis_Comparison.ipynb`** and run all cells.  
This notebook:
- Reevaluates DL (AUC, F1)
- Computes RL EPV (and FQE if available)
- Displays policy disagreement examples
- Generates a final summary report for submission

**Outputs:**
- Final metrics (AUC, F1, EPV, approval rate)  
- Policy disagreement table  
- One-page final report (summary text block)

---

## üíª Command-Line Quickstart (Optional)

To run everything from a single notebook (Task 2‚Äì4):

1. Ensure `data/loan_clean_subset.csv` exists.  
2. Open **`notebooks/Task4_Analysis_Comparison.ipynb`** and Run All.  

This will:
- Train the PyTorch MLP (DL) and print AUC/F1  
- Train the CQL (RL) agent and print EPV  
- Display policy disagreements and final summary

---

## üß© Troubleshooting

| Issue | Solution |
|-------|-----------|
| **TensorFlow/d3rlpy/gymnasium errors on Python 3.13** | Use **Python 3.11** and create the `loan-rl` Conda environment as shown above. |
| **ModuleNotFoundError: gymnasium.wrappers.time_limit** | Install `gymnasium[classic-control]==0.29.1`. |
| **`d3rlpy.fit()` signature mismatch (expects n_steps)** | Use the positional form:<br>`algo.fit(train_dataset, 100_000)` ‚Äî already handled in notebooks. |
| **GPU not used** | CPU execution works fine. If CUDA is available, PyTorch detects it automatically. |
| **Missing loan_amnt/int_rate in clean file** | Re-run Task 1 to regenerate the dataset with these columns. |

---

## üìä Expected Outputs

- **DL Model:** ROC-AUC, F1, classification report, confusion matrix  
- **RL Model:** Estimated Policy Value (EPV), approval rate, approved-paid and approved-defaulted counts  
- **Disagreement Table:** Cases where DL and RL policies differ  
- **Final Report:** Concise text summary of results and recommendations

---

## üìö Citations & Libraries

- **PyTorch** ‚Äî Deep Learning modeling  
- **scikit-learn** ‚Äî Preprocessing, metrics, train/test split  
- **d3rlpy** ‚Äî Offline RL (CQL), Optional OPE (FQE)  
- **gymnasium** ‚Äî RL environment dependencies  

---

## ‚öñÔ∏è License

This project is intended **solely for educational and internal evaluation** purposes.

---

# üßæ requirements.txt

```
numpy<2.0
pandas>=2.0.0
scikit-learn>=1.3.0
matplotlib>=3.7.0
joblib>=1.3.0

# Deep Learning (PyTorch)
torch==2.4.1

# Offline RL (Conservative Q-Learning) & dependencies
d3rlpy==2.4.0
gymnasium[classic-control]==0.29.1

# Jupyter kernel support (optional)
ipykernel>=6.29.0
```
